\documentclass{article}
\usepackage[left=3cm,right=3cm,top=3cm,bottom=2cm]{geometry}
\usepackage{amsmath, amssymb, amsthm, enumitem, tikz}
\usepackage{mathtools} %For arrows with text above and below%
  
\newcommand{\Z}{\mathbb{Z}}    % integers
\newcommand{\N}{\mathbb{N}}    % naturals
\newcommand{\R}{\mathbb{R}}    % reals
\newcommand{\E}{\mathbb{E}}    % expectation
\renewcommand{\P}{\mathbb{P}}  % probability
\DeclareMathOperator{\sd}{sd}
\DeclareMathOperator{\var}{var}
\DeclareMathOperator{\cov}{cov}

% distributions
\DeclareMathOperator{\Normal}{Normal}
\DeclareMathOperator{\Poisson}{Poisson}
\DeclareMathOperator{\Beta}{Beta}
\DeclareMathOperator{\Binom}{Binomial}
\DeclareMathOperator{\Gam}{Gamma}
\DeclareMathOperator{\Exp}{Exponential}
\DeclareMathOperator{\Cauchy}{Cauchy}
\DeclareMathOperator{\Unif}{Unif}
\DeclareMathOperator{\Dirichlet}{Dirichlet}
  
\newcommand{\given}{\;\vert\;}
  
\theoremstyle{definition}
\newtheorem{thm}{Theorem}[section]
\newtheorem{prop}[thm]{Proposition}
\newtheorem{coro}[thm]{Corollary}
\newtheorem{lemma}[thm]{Lemma}
\newtheorem{defn}[thm]{Definition}
\newtheorem{exmp}[thm]{Example}
\newtheorem{rmk}[thm]{Remark}
\newtheorem{exer}[thm]{Exercise}
\newtheorem{nota}[thm]{Notation}
\newtheorem*{note}{Note}
\newtheorem*{sol}{Solution}


\begin{document}

%%%%%% LECTURE 3 %%%%%%

\section{A Gaussian Process on $L^2(\R)$}\

We will construct a process on $L^2(\R)$ by taking a limit of a simpler process on the space of sequences $\ell^2(\R):=\left\{(a_k)_{k\in\Z}:\sum_{k\in\Z}|a_k|^2<\infty,a_k\in\R\right\}$. Given a set of independent random variables $\{X_k\}_{k\in\Z}$ satisfying $X_k\sim N(0,1)$ (think of each $X_k$ as ``noise" at the integer $k$), define $Z(a):=\sum_{k\in\Z}a_kX_k$ for $a=(a_k)\in\ell^2(\R)$. The collection $\{Z(a)\}_{a\in\ell^2(\R)}$ is a Gaussian process that is centered (i.e., $\E[Z(a)]=0$) and satisfies $$\var[Z(a)]=\cov\left[\sum_{k\in\Z}a_kX_k,\sum_{l\in\Z}a_lX_l\right]=\sum_{k,l\in\Z}a_ka_l\cov[X_k,X_l]=\sum_{k\in\Z}a_k^2$$ for each $a\in\ell^2(\R)$. Now recall $L^2(\R):=\{f:\R\to\R:\int_{-\infty}^\infty(f(x))^2\,dx<\infty\}$. Given $f\in L^2(\R)$, define a sequence of random variables $(Z^{(N)}(f))_{N\in\N}$ by $$Z^{(N)}(f):=\frac{1}{\sqrt{N}}\sum_{k\in\Z}f\left(\frac{k}{N}\right)X_k.$$ It can be shown that $Z^{(N)}(f)$ converges in distribution to a random variable which we will call $Z(f)$. It can be also shown that the collection $\{Z(f)\}_{f\in L^2(\R)}$ is a centered Gaussian process satisfying $\cov[Z(f),Z(g)]=\int_{-\infty}^\infty f(x)g(x)\,dx$. It turns out that these properties (centered Gaussian and satisfying the above covariance formula) characterize $\{Z(f)\}_{f\in L^2(\R)}$, so we might as well define it this way:

\begin{defn}
The collection $\{Z(f)\}_{f\in L^2(\R)}$ is the centered Gaussian process on $L^2(\R)$ satisfying $$\cov[Z(f),Z(g)]=\int_{-\infty}^\infty f(x)g(x)\,dx.$$
\end{defn} 

Note that $Z(af+bg)=aZ(f)+bZ(g)$ for all $a,b\in\R$ and all $f,g\in L^2(\R)$, so $f\mapsto Z(f)$ defines a linear map from $L^2(\R)$ to the space of random variables. The random variable $Z(f)$ is called the \emph{stochastic integral} of $f$, denoted $$Z(f)=:\int_{-\infty}^\infty f(t)\,dW_t,$$ and the symbol $dW_t$ is interpreted as ``white noise" that weights the value of $f(t)$. Assuming it is known that $\cov[dW_s,dW_t]=\delta_s(t)\,ds\,dt$, where $\delta_s$ is the Dirac $\delta$-function centered at $s$, we can recover the covariance formula using stochastic integral notation:
\begin{align*}
\cov[Z(f),Z(g)]&=\cov\left[\int_{-\infty}^\infty f(s)dW_s,\int_{-\infty}^\infty g(t)dW_t\right]\\
&=\int_{-\infty}^\infty\int_{-\infty}^\infty f(s)g(t)\cov[dW_s,dW_t]=\int_{-\infty}^\infty f(t)g(t)\,dt.
\end{align*}

The following examples show that some stochastic processes can be defined as stochastic integrals.

\begin{exmp}[Brownian motion] If we define $B_t:=Z(\mathbf{1}_{[0,t)})=\int_0^tdW_s$, then the process $\{B_t\}_{t\in[0,\infty)}$ is a Brownian motion. Indeed,
\begin{enumerate}[label=(\roman*)]
\item $B_t$ is Gaussian,
\item $\cov[B_s,B_t]=\int_{-\infty}^\infty\mathbf{1}_{[0,s)}(u)\mathbf{1}_{[0,t)}(u)\,du=\int_0^{\min\{s,t\}}\,du=\min\{s,t\}$ (and hence $\var[B_t]=t$),
\item if $u<v\leq s<t$ then $$\cov[B_t-B_s,B_v-B_u]=\int_{-\infty}^\infty\mathbf{1}_{[s,t)}(x)\mathbf{1}_{[u,v)}(x)\,dx=\int_{-\infty}^\infty\mathbf{1}_{[s,t)\cap[u,v)}(x)\,dx=0.$$
\end{enumerate}
\end{exmp}

\begin{exmp}[Energy]
Define a process on $\{Y_t\}_{t\in\R}$ by $Y_t:=Z\left(e^{-(t-s)}\mathbf{1}_{(-\infty,t]}(s)\right)=\int_{-\infty}^\infty e^{-(t-s)}\,dW_s.$ Then
\begin{enumerate}[label=(\roman*)]
\item $Y_t$ is Gaussian,
\item for $s\leq t$ we have $$\cov[Y_s,Y_t]=\int_{-\infty}^\infty e^{-(s-u)}\mathbf{1}_{(-\infty,s]}(u)^2e^{-(t-u)}\mathbf{1}_{(-\infty,t]}(u)^2\,du=e^{-s-t}\int_{-\infty}^se^{2u}\,du=\frac{e^{-(t-s)}}{2},$$ and hence $\var[Y_t]=\frac{1}{2}$.
\end{enumerate}
\end{exmp}

\begin{exer}
At each position $x$ along a river, a number of mosquitos are born, independently for each $x$. Let $X_x$ be the difference between this number and the average number of mosquitos born (averaged over all locations $x\in(-\infty,\infty)$) and suppose $\var[X_x]=\sigma^2$ for all $x$. After birth, each mosquito disposes with a probability of density of moving displacement $d$ given by $$K(d):=\frac{1}{\pi(1+d^2)}.$$ Describe the density of mosquitos using a Gaussian process and compute $$\cov[(\#\text{ of mosquitos at }x),(\#\text{ of mosquitos at }y)].$$
\end{exer}

%%%%%% LECTURE 4 %%%%%%

\section{A Different Construction of $Z(f)$}

\begin{defn}
Define the \emph{Haar basis} $\{h_{n,k}\}$ for $L^2([0,1])$ by $h_{0,0}\equiv 1$ and $$h_{n,k}(t):=\begin{cases}2^{(n-1)/2}&\text{if }\frac{2k}{2^n}\leq t<\frac{2k+1}{2^n},\\-2^{(n-1)/2}&\text{if }\frac{2k+1}{2^n}\leq t<\frac{2k+2}{2^n},\\0&\text{otherwise,}\end{cases}$$ if $n\geq 1$ and $0\leq k\leq 2^{n-1}$.
\end{defn}

It is easily verified that $\{h_{n,k}\}$ is an orthonormal set in $L^2([0,1])$, i.e., $$\int_0^1h_{n,k}(t)h_{m,\ell}(t)\,dt=\begin{cases}1&\text{if $n=m$ and $k=\ell$},\\0&\text{otherwise}.\end{cases}$$ Now suppose $\{Z_{n,k}\}$ is a collection of independent $N(0,1)$ variables and $f\in L^2([0,1])$ expands as $$f(t)=\sum_{n=0}^\infty\sum_{k=0}^{2^{n-1}}a_{n,k}h_{n,k}(t),\quad\text{where}\quad a_{n,k}=\int_0^1f(t)h_{n,k}(t)\,dt.$$ The random variable $Z(f)$ defined by $$Z(f):=\sum_{n=0}^\infty\sum_{k=0}^{2^{n-1}}a_{n,k}Z_{n,k}$$ gives a Gaussian process $\{Z(f)\}$ for which $$\cov[Z(f),Z(g)]=\sum_{n=0}^\infty\sum_{k=0}^{2^{n-1}}\sum_{m=0}^\infty\sum_{\ell=0}^{2^{m-1}}a_{n,k}b_{m,\ell}\cov[Z_{n,k},Z_{m,k}]=\sum_{n=0}^\infty\sum_{k=0}^{2^{n-1}}a_{n,k}b_{n,k}.$$ In particular we have $$\var[Z(f)]=\sum_{n=0}^\infty\sum_{k=0}^{2^{n-1}}a_{n,k}^2=\int_0^1f(t)^2\,dt,$$ where the latter equality is given by Parseval's identity (see page 85 of Rudin's \emph{Real \& Complex Analysis}). In the following example we see that Brownian motion can again be constructed by applying $Z$ to a particular collection of functions.
 
\begin{exmp}[L\'evy's Construction]
Setting $B_t=Z(\mathbf{1}_{[0,t)})$ for $t\in[0,1]$, we have $$a_{n,k}=\int_0^1\mathbf{1}_{[0,t)}h_{n,k}(s)\,ds=\int_0^th_{n,k}(s)\,ds$$ and $$B_t=\sum_{n=0}^\infty\sum_{k=0}^{2^{n-1}}Z_{n,k}\int_0^th_{n,k}(s)\,ds.$$ Since $Z$ is linear (with respect to $f$), for $s\leq t$ we have $$\cov[B_t-B_s]=\cov[Z(\mathbf{1}_{[0,t)})-Z(\mathbf{1}_{[0,s)})]\cov[Z(\mathbf{1}_{[s,t)})]=\|\mathbf{1}_{[s,t)}\|_2^2=t-s.$$
\end{exmp}

\section{The Brownian Bridge}
Let $U_t=(B_t\mid B_1=0)$ for $0\leq t\leq 1$. Since $B_1=Z_{0,0}$, we have $$U_t=(B_t\mid Z_{0,0}=0)=\sum_{n=1}^\infty\sum_{k=1}^{2^{n-1}}Z_{n,k}\int_0^th_{n,k}(s)\,ds=B_t-tB_t.$$


\end{document}